---
title: "Data provenance and bowerbird"
author: "Ben Raymond, Michael Sumner"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data provenance and bowerbird}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Bowerbird will maintain a local collection of data files, sourced from external data providers. When one does an analysis using such files, it is useful to know *which* files were used, and the *provenance* of those files. This information will assist in making analyses reproducible.

It is necessary to understand that bowerbird knows very little about the data files that it maintains. For a give data source, it knows the URL and flags to pass to `wget`, and some basic metadata (primarily intended to be read by the user). This is by design: the heavy lifting involved in mirroring a remote data source is (for most data sources) taken care of by `wget`. This makes the bowerbird code simpler, and importantly, means that it is generally straightforward for a user to define a new data source.

The downside of this is that bowerbird can't necessarily the two questions above in specific detail.

Say that we are using data from a data source that contains many separate files, one per day, spanning many years, and we run an analysis that uses only files from the year 1999. Ideally we would like to know exacty which files these were, but without specific knowledge of how the data source is structured this is impossible. Analogous uncertainty exists with data sources that split geographic space across files, or split different parameters across files.

Despite this, bowerbird can provide some general information to assist with these issues.

## Which files were used in an analysis

Say we have a bowerbird config:
```{r eval=FALSE}
cf <- bb_config("/some/local/path") %>% add(src)
```
We can ask bowerbird where these files are stored locally:
```{r eval=FALSE}
data_source_dir(cf[1,])
```

This directory will contain all of the files associated with that data source, not just the files used in a particular analysis. (In extreme cases this directory might even contain files from other data sources, although this should only happen if multiple data sources have overlapping `source_url`s, which ought not to be a common occurrence.)

So while this directory is not a minimal set of files needed to reproduce an analysis, it does at least contain that set, and could be used to store those files in an online repository that assigns DOIs (such as figshare, see https://cran.r-project.org/package=rfigshare, or zenodo), or bundle into a docker image (see e.g containerit https://github.com/o2r-project/containerit).

### Refining the list of files

Ascertaining exactly which files were used is better handled by the code being used to do the file-reading and analysis, not by the repository-management (bowerbird) code.


... unaware of general solutions

The recordr package (https://github.com/NCEAS/recordr) will collect info about which files were read or written. However, this only works for file types that have read functions implemented in recordr. At the time of writing, this does not cover netcdf files or other files typically used for environmental data, and so its application in that domain is likely to be of limited value.

... other package-specific solutions, e.g. raadtools

## The provenance of files used in an analysis

Provenance: where the files came from, when they were downloaded, their version, etc.

The `bb_fingerprint` function, given a data repository configuration, will return the timestamp of download and hashes of all files associated with its data sources. Thus, for all of these files, we have established where they came from (the data source ID), when they were downloaded, and a hash so that later versions of those files can be compared to detect changes.

The data source ID should be its DOI (if it has one, or some other unique identifier if not). A data set that has a DOI should be peristent (accessible for long term use), and the DOI should uniquely identify the data resource that it is assigned to. When a data set changes in a substantial manner, and/or it is necessary to identify both the original and the changed material, a new DOI should be assigned. Thus, knowing the DOI gives some indication of the data provenance. However,

... distinct state of data set might/not follow DOI uniqueness


MODIS Composite Based Maps of East Antarctic Fast Ice Coverage http://doi.org/10.4225/15/5667AC726B224
NSIDC SMMR-SSM/I Nasateam near-real-time sea ice concentration http://doi.org/10.5067/U8C09DWVX9LM


## Context (to be removed)

how is the original origin, date, and version recorded and maintained? How can a version of analysis be linked to to a version of the data if data is maintained separately from analysis code?

Some possible solutions would be

- Being able to maintain multiple versions or version-controlled repositories of these local stores. (possibly using git, git-lfs, AWS versioned stores, or just a directory that is "frozen" to link to an analysis)
- Relying on the data source when the provider does versioning (e.g., different versions of files may be available to sync, such as via versioned DOIs or URIs), and allowing the user to change the version of the source.
